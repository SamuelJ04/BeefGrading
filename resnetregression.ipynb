{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model for USDA Beef Grading\n",
    "\n",
    "This model will use pytoch and resnet to make a CNN regression model (convolutional neural network)\n",
    "hay que mantener 5 pasos\n",
    "1. cargando imagenes y procesarlos\n",
    "2. modificar el modelo resnet para la regresion\n",
    "3. entrenar el model\n",
    "4. evaluar modelo (root mean square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"Data/Cleaned JPEG Images/Cargill Dodge 07 20 22 Plant/Images\"\n",
    "excel_path = \"Data/Cleaned JPEG Images/Cargill Dodge 07 20 22 Plant/data.xlsx\"\n",
    "\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "#ID_to_score = dict(zip(df['CarcassId'], df['Average']))\n",
    "ID_to_score = {f\"{carcass_id}.jpg\": avg for carcass_id, avg in zip(df['CarcassId'], df['Average'])}\n",
    "\n",
    "#print(ID_to_score.keys())\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform =None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = list(ID_to_score.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_name)\n",
    "        score = ID_to_score[self.image_filenames[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, score\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(image_folder, transform=transform)\n",
    "dataloader= DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "   #load pretrained resnet model\n",
    "resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features,1) #output 1 for regression\n",
    "\n",
    "#LOAD IN THE GPU (MUY IMPORTANTE NO OLVIDES)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/65], Loss: 182630.7691\n",
      "Epoch [2/65], Loss: 175358.0590\n",
      "Epoch [3/65], Loss: 173416.7361\n",
      "Epoch [4/65], Loss: 166824.3038\n",
      "Epoch [5/65], Loss: 161668.0208\n",
      "Epoch [6/65], Loss: 158619.5521\n",
      "Epoch [7/65], Loss: 152270.8750\n",
      "Epoch [8/65], Loss: 145265.3802\n",
      "Epoch [9/65], Loss: 138760.8733\n",
      "Epoch [10/65], Loss: 132180.5373\n",
      "Epoch [11/65], Loss: 124426.1970\n",
      "Epoch [12/65], Loss: 118560.6510\n",
      "Epoch [13/65], Loss: 111897.7569\n",
      "Epoch [14/65], Loss: 104566.4106\n",
      "Epoch [15/65], Loss: 98065.2439\n",
      "Epoch [16/65], Loss: 90692.0026\n",
      "Epoch [17/65], Loss: 85449.9618\n",
      "Epoch [18/65], Loss: 78205.3880\n",
      "Epoch [19/65], Loss: 71826.1432\n",
      "Epoch [20/65], Loss: 66184.8650\n",
      "Epoch [21/65], Loss: 59812.9666\n",
      "Epoch [22/65], Loss: 54018.3277\n",
      "Epoch [23/65], Loss: 49244.5543\n",
      "Epoch [24/65], Loss: 43684.5747\n",
      "Epoch [25/65], Loss: 38916.6376\n",
      "Epoch [26/65], Loss: 34297.5601\n",
      "Epoch [27/65], Loss: 31275.2339\n",
      "Epoch [28/65], Loss: 26628.8257\n",
      "Epoch [29/65], Loss: 23772.1467\n",
      "Epoch [30/65], Loss: 19936.9427\n",
      "Epoch [31/65], Loss: 17755.9965\n",
      "Epoch [32/65], Loss: 14376.4768\n",
      "Epoch [33/65], Loss: 11889.5897\n",
      "Epoch [34/65], Loss: 9632.9597\n",
      "Epoch [35/65], Loss: 7891.7827\n",
      "Epoch [36/65], Loss: 6763.3697\n",
      "Epoch [37/65], Loss: 5457.3654\n",
      "Epoch [38/65], Loss: 4426.8686\n",
      "Epoch [39/65], Loss: 3435.4795\n",
      "Epoch [40/65], Loss: 2916.3967\n",
      "Epoch [41/65], Loss: 2370.5699\n",
      "Epoch [42/65], Loss: 2323.2648\n",
      "Epoch [43/65], Loss: 2251.0015\n",
      "Epoch [44/65], Loss: 2351.4160\n",
      "Epoch [45/65], Loss: 2162.9988\n",
      "Epoch [46/65], Loss: 2183.3844\n",
      "Epoch [47/65], Loss: 2143.3813\n",
      "Epoch [48/65], Loss: 2047.1478\n",
      "Epoch [49/65], Loss: 1889.0325\n",
      "Epoch [50/65], Loss: 1981.5079\n",
      "Epoch [51/65], Loss: 1929.8396\n",
      "Epoch [52/65], Loss: 2026.7425\n",
      "Epoch [53/65], Loss: 1706.8339\n",
      "Epoch [54/65], Loss: 1891.9381\n",
      "Epoch [55/65], Loss: 2145.1531\n",
      "Epoch [56/65], Loss: 1665.1060\n",
      "Epoch [57/65], Loss: 1635.4079\n",
      "Epoch [58/65], Loss: 1550.1026\n",
      "Epoch [59/65], Loss: 1474.8097\n",
      "Epoch [60/65], Loss: 1392.4756\n",
      "Epoch [61/65], Loss: 1329.1214\n",
      "Epoch [62/65], Loss: 1518.1655\n",
      "Epoch [63/65], Loss: 1477.8740\n",
      "Epoch [64/65], Loss: 1281.6206\n",
      "Epoch [65/65], Loss: 1184.6555\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr = 0.001)\n",
    "scheduler = StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "num_epochs = 65\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss =0.0\n",
    "    for images, scores in dataloader:\n",
    "        images = images.to(device)\n",
    "        scores = scores.to(device).view(-1,1).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, scores)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "    \n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 31.2301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.230062"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, scores in dataloader:\n",
    "            images = images.to(device)\n",
    "            scores = scores.to(device).view(-1,1).float()\n",
    "            outputs = model(images)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_scores.append(scores.cpu().numpy())\n",
    "            \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    mse = mean_squared_error(all_scores, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    return rmse\n",
    "\n",
    "evaluate_model(resnet, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc / Junk Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
